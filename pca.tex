
%\begin{frame}{principal component analysis}{introduction}
    %\begin{itemize}
        %\item   linear transformation
        %\item   resulting principal components are 
					%\begin{itemize}
						%\item uncorrelated
						%\item	sorted (by variance)
					%\end{itemize}
    %\end{itemize}
%\end{frame}

\begin{frame}{principal component analysis}{introduction}
				\begin{itemize}
						\item   \textbf{objective}
								\begin{itemize}
										\item   map features to new coordinate system
												\begin{equation*}\label{eq:pca_an}
														\vec{u}(n) = \mat{T}^\mathrm{T}\cdot\vec{v}(n) 
												\end{equation*}
												\begin{itemize}
														\item   $\vec{u}(n)$: transformed features (same dimension as input $\vec{v}(n)$)
														\item   $\mat{T}$: transformation matrix ($\mathcal{F}_v\times\mathcal{F}_v$)	
																\begin{equation*}
																		\mat{T} =   \left[ 
																										\begin{array}{cccc}
																										\vec{c}_0 & \vec{c}_1 & \ldots & \vec{c}_{\mathcal{F}-1}\\
																										\end{array}  
																								\right] 
																\end{equation*}
												\end{itemize}
								\end{itemize}
						\item<2->   \textbf{properties}
								\begin{itemize}
										\item	$\vec{c}_0$ points in the direction of  highest \emph{variance} (sorted by eigenvalue)
										\item<2->	variance concentrated in as few output components as possible
										\item<2->	$\vec{c}_i$ orthogonal
														\begin{equation*}
																\vec{c}_i^\mathrm{T}\cdot \vec{c}_j = 0\quad \forall\enspace i \neq j
														\end{equation*}
										\item<2->	transformation is invertible
														\begin{equation*}\label{eq:pca_syn}
																\vec{v}(n) = \mat{T}\cdot\vec{u}(n)
														\end{equation*}
								\end{itemize}
				\end{itemize}
\end{frame}

\begin{frame}{principal component analysis}{calculation}
	%\vspace{-4mm}
	\begin{columns}
	\column{.5\linewidth}
		\textbf{calculation} of the transformation matrix
		\begin{enumerate}
							\item normalize input data
			\item	compute covariance matrix $\mat{R}$
									\begin{equation*}
					\mat{R} = \mathcal{E}\lbrace(V-\mathcal{E}\lbrace V\rbrace)(V-\mathcal{E}\lbrace V\rbrace)\rbrace%\frac{1}{\mathcal{F}-1}\cdot \left(\vec{v} - \vec{\mu}_v\right)\left(\vec{v}^\mathrm{T} - \vec{\mu}^\mathrm{T}_v\right)
				\end{equation*}
			\item	compute eigenvectors and eigenvalues
			\item	sort eigenvectors by eigenvalue and use as axes for the new coordinate system
							\item   (remove irrelevant components and) apply transformation matrix
		\end{enumerate}
	\column{.5\linewidth}
		\figwithmatlab{Pca}
		
		\visible<2->{
		\textbf{properties}:
		\begin{itemize}
				\item linear transformation
        \item   resulting principal components are 
					\begin{itemize}
						\item uncorrelated
						\item	sorted (by variance)
					\end{itemize}
    \end{itemize}
		}
	\end{columns}
\end{frame}

\begin{frame}{principal component analysis}{drawbacks}
	\vspace{-7mm}
		\begin{columns}
		\column{.6\linewidth}
		\begin{itemize}
				\item 	no \textbf{component interpretability}: principal components are 'unintuitive' combinations of input features
				\item<2->		\textbf{linear data} only: nonlinear relationships between inputs are ignored
				\item<3->		\textbf{sorting criteria} not necessarily task-relevant 
				\item<4->		can be \textbf{affected by outliers}
				\item<5->		unclear \textbf{optimum number} of resulting components
						\begin{itemize}
							\item 	eigenvalue $> 1$
							\item 	cumulative variance $>95\%$
							\item		elbow in scree plot
						\end{itemize}
		\end{itemize}
		\column{.4\linewidth}
		
		%TODO: copilot interpretability
		
		\only<2>{
				\figwithref{pca_nonlinear}{\href{https://medium.com/@sakethyalamanchili/pca-vs-kpca-which-technique-will-work-best-for-your-data-7b4baf83ebcb}{medium.com/@sakethyalamanchili/pca-vs-kpca-which-technique-will-work-best-for-your-data-7b4baf83ebcb}}
		}
		
		\only<3>{
				\figwithref{pca_pitfall}{\href{https://ekamperi.github.io/mathematics/2021/02/23/pca-limitations.html}{ekamperi.github.io/mathematics/2021/02/23/pca-limitations.html}}
		}
		
		\only<4>{
				\figwithref{outlier}{\href{https://stats.stackexchange.com/questions/259806/anomaly-detection-using-pca-reconstruction-error}{stats.stackexchange.com/questions/259806/anomaly-detection-using-pca-reconstruction-error}}
		}
		
		\only<5>{
				\figwithref{pca_scree}{\href{https://blog.dailydoseofds.com/p/how-many-dimensions-should-you-reduce}{blog.dailydoseofds.com/p/how-many-dimensions-should-you-reduce}}
		}
		
		\end{columns}
\end{frame}

 
		